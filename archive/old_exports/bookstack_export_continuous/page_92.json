{
  "id": 92,
  "title": "Hybrid Embedding-LLM Tool Selection",
  "slug": "hybrid-embedding-llm-tool-selection",
  "url": "https://knowledge.oculair.ca/books/unknown/page/hybrid-embedding-llm-tool-selection",
  "updated_at": "2025-04-12T22:26:46.000000Z",
  "body_html": "<h1 id=\"bkmrk-hybrid-tool-selectio\">Hybrid Tool Selection Approach: Combining Embeddings and Small LLMs</h1>\n<p id=\"bkmrk-this-document-outlin\">This document outlines a hybrid approach to dynamic tool loading for Letta agents, combining vector embeddings with small language models to achieve optimal tool selection when managing hundreds of tools.</p>\n<h2 id=\"bkmrk-overview\">Overview</h2>\n<p id=\"bkmrk-the-hybrid-approach-\">The hybrid approach leverages the strengths of both vector embeddings (for fast initial filtering) and small language models (for refined selection). This provides an optimal balance of speed, accuracy, and scalability, particularly when dealing with very large tool collections.</p>\n<h2 id=\"bkmrk-implementation-flow\">Implementation Flow</h2>\n<ol id=\"bkmrk-preprocess-tool-desc\">\n<li>Preprocess tool descriptions to generate vector embeddings</li>\n<li>Store embeddings in a vector database</li>\n<li>For each query:\na. Convert user query to a vector embedding\nb. Retrieve top-k semantically similar tools using vector similarity search\nc. Use a small LLM to refine the selection from the shortlisted tools\nd. Attach selected tools to Letta agent\ne. Maintain previously used tools for context continuity</li>\n</ol>\n<h2 id=\"bkmrk-code-implementation\">Code Implementation</h2>\n<pre id=\"bkmrk-def-setup_hybrid_too\"><code class=\"language-python\">def setup_hybrid_tool_selection(all_tools, embedding_model=\"text-embedding-ada-002\"):\n    \"\"\"Set up the vector database for the hybrid approach\"\"\"\n    \n    # Create a description for each tool\n    tool_texts = []\n    for tool in all_tools:\n        tool_text = f\"Tool Name: {tool.name}\\nDescription: {tool.description}\\nUsage Examples: {tool.examples}\"\n        tool_texts.append(tool_text)\n    \n    # Generate embeddings\n    embeddings = embedding_model.embed_documents(tool_texts)\n    \n    # Store in vector database with tool IDs as metadata\n    vector_db = VectorStore()\n    for i, embedding in enumerate(embeddings):\n        vector_db.add(\n            embedding=embedding,\n            metadata={\"tool_id\": all_tools[i].id}\n        )\n    \n    return vector_db\n\ndef refine_tool_selection(query, candidate_tools, small_model=\"command-r7b\"):\n    \"\"\"Use small LLM to refine the selection from candidate tools\"\"\"\n    \n    # Create concise descriptions of candidate tools\n    tool_descriptions = [\n        f\"ID: {tool.id}, Name: {tool.name}, Description: {tool.description}\" \n        for tool in candidate_tools\n    ]\n    \n    # Create prompt for the small model\n    prompt = f\"\"\"\n    Query: {query}\n    \n    Pre-selected Tools:\n    {'\\n'.join(tool_descriptions)}\n    \n    Select the tool IDs that would be most helpful for answering this query.\n    Return only the relevant tool IDs as a comma-separated list.\n    \"\"\"\n    \n    # Use small model to select tools\n    small_model = get_model(small_model)\n    selected_tool_ids = small_model.generate(prompt).strip()\n    \n    # Parse response\n    tool_ids = [id.strip() for id in selected_tool_ids.split(',')]\n    refined_tools = [tool for tool in candidate_tools if tool.id in tool_ids]\n    \n    return refined_tools\n\ndef select_tools_hybrid(query, vector_db, all_tools, small_model=\"command-r7b\", \n                        previous_tools=None, top_k=20, max_tools=5):\n    \"\"\"Select tools using hybrid embedding + LLM approach\"\"\"\n    \n    # Step 1: Get embedding for the query\n    query_embedding = embedding_model.embed_query(query)\n    \n    # Step 2: Retrieve candidate tools using vector similarity\n    similar_tool_results = vector_db.similarity_search(\n        query_embedding,\n        top_k=top_k  # Retrieve more candidates than needed\n    )\n    \n    # Step 3: Get candidate tool objects\n    tool_ids = [result.metadata[\"tool_id\"] for result in similar_tool_results]\n    candidate_tools = [tool for tool in all_tools if tool.id in tool_ids]\n    \n    # Step 4: Use small LLM to refine the selection\n    refined_tools = refine_tool_selection(query, candidate_tools, small_model)\n    \n    # Step 5: Apply maximum tool limit if needed\n    if len(refined_tools) &gt; max_tools:\n        refined_tools = refined_tools[:max_tools]\n    \n    # Step 6: Include previously used tools for continuity\n    if previous_tools:\n        final_tools = list(set(refined_tools + previous_tools))\n        # If we still have too many tools, prioritize the new selections\n        if len(final_tools) &gt; max_tools:\n            # Keep all new refined tools and add previous tools until we hit the limit\n            remaining_slots = max_tools - len(refined_tools)\n            if remaining_slots &gt; 0:\n                previous_not_in_refined = [t for t in previous_tools if t not in refined_tools]\n                final_tools = refined_tools + previous_not_in_refined[:remaining_slots]\n            else:\n                final_tools = refined_tools[:max_tools]\n    else:\n        final_tools = refined_tools\n    \n    return final_tools\n\ndef handle_query_hybrid(query, vector_db, all_tools, small_model=\"command-r7b\", previous_tools=None):\n    # Step 1: Select relevant tools using hybrid approach\n    relevant_tools = select_tools_hybrid(\n        query, vector_db, all_tools, small_model, previous_tools\n    )\n    \n    # Step 2: Attach tools to Letta agent\n    agent = attach_tools_to_agent(\"your-letta-agent-id\", relevant_tools)\n    \n    # Step 3: Let agent handle the query with available tools\n    response = prompt_agent(agent.id, query)\n    \n    return response, relevant_tools  # Return tools for next query\n</code></pre>\n<h2 id=\"bkmrk-advantages\">Advantages</h2>\n<ul id=\"bkmrk-scalability%3A-the-vec\">\n<li><strong>Scalability</strong>: The vector search component allows efficient handling of thousands of tools</li>\n<li><strong>Accuracy</strong>: The LLM component provides more nuanced selection than embeddings alone</li>\n<li><strong>Efficiency</strong>: Only runs the LLM on a pre-filtered subset of tools, reducing costs</li>\n<li><strong>Speed</strong>: Faster than running LLM inference across all tools</li>\n<li><strong>Context-Awareness</strong>: Maintains previously used tools for continuity</li>\n</ul>\n<h2 id=\"bkmrk-limitations\">Limitations</h2>\n<ul id=\"bkmrk-complexity%3A-more-com\">\n<li><strong>Complexity</strong>: More complex setup than either approach alone</li>\n<li><strong>Dependent Systems</strong>: Requires both vector database and LLM components</li>\n<li><strong>Tuning Required</strong>: Need to optimize parameters like top-k values</li>\n<li><strong>Maintenance</strong>: Both embeddings and LLM components need to be maintained</li>\n</ul>\n<h2 id=\"bkmrk-implementation-consi\">Implementation Considerations</h2>\n<ol id=\"bkmrk-parameter-tuning%3A-ad\">\n<li>\n<p><strong>Parameter Tuning</strong>:</p>\n<ul>\n<li>Adjust top-k for initial vector retrieval based on tool library size</li>\n<li>Balance between retrieving enough candidates without overwhelming the LLM</li>\n</ul>\n</li>\n<li>\n<p><strong>Tool Prioritization</strong>:</p>\n<ul>\n<li>Consider adding a priority mechanism for tools that should always be considered</li>\n<li>Implement a deprecation mechanism for tools that are rarely used</li>\n</ul>\n</li>\n<li>\n<p><strong>Caching</strong>:</p>\n<ul>\n<li>Cache both vector search results and LLM refinement results for similar queries</li>\n<li>Implement an LRU (Least Recently Used) cache to optimize memory usage</li>\n</ul>\n</li>\n<li>\n<p><strong>Monitoring and Analytics</strong>:</p>\n<ul>\n<li>Track which tools are frequently selected together</li>\n<li>Monitor selection accuracy by comparing selected tools with actually used tools</li>\n</ul>\n</li>\n<li>\n<p><strong>Fallback Mechanisms</strong>:</p>\n<ul>\n<li>If vector search fails, fall back to LLM-only selection</li>\n<li>If LLM refinement fails, use the top vector search results directly</li>\n</ul>\n</li>\n</ol>\n<p id=\"bkmrk-this-hybrid-approach\">This hybrid approach provides the best balance of performance, accuracy, and scalability for dynamic tool loading in Letta agents, especially when dealing with very large tool libraries.</p>\n",
  "tags": [],
  "book": "Dynamic Tool Loading for Letta",
  "chapter": ""
}