{
  "id": 122,
  "title": "Chapter 1: Optimized Web Research Workflow",
  "slug": "chapter-1-optimized-web-research-workflow",
  "url": "https://knowledge.oculair.ca/books/unknown/page/chapter-1-optimized-web-research-workflow",
  "updated_at": "2025-05-13T06:31:15.000000Z",
  "body_html": "<pre id=\"bkmrk-%23-chapter-1%3A-optimiz\"><code class=\"language-markdown\"># Chapter 1: Optimized Web Research Workflow\n\n## 1. Introduction\n\nThis document outlines an optimized workflow for handling web research requests within the Letta agent environment. The primary goals are to minimize context window usage by avoiding the direct injection of verbose search results into the main conversation, while ensuring comprehensive data capture for long-term recall and improving the quality of information ingested into the Graphiti knowledge graph.\n\n## 2. Workflow Steps\n\nThe process involves the agent, a web search tool, a temporary memory block, an external Node.js service, a Quality Control (QC) LLM, and the Graphiti knowledge graph.\n\n1.  **Research Request:** The user makes a research request to the Letta agent.\n2.  **Tool Execution &amp; Initial Storage:**\n    *   The agent invokes the designated web search tool.\n    *   The tool performs the search and stores the *full, raw search results* into a dedicated, uniquely identified memory block, referred to as the \"Research Cache Block.\"\n3.  **Contextual Summary:**\n    *   The tool returns a brief summary (e.g., page titles, key snippets) and a reference (the unique ID of the Research Cache Block) to the agent. This summary is then presented to the user in the chat.\n4.  **External Processing Trigger (Direct API Call):**\n    *   After successfully creating/updating the Research Cache Block and returning the summary, the web search tool makes a direct API call (e.g., POST request) to a predefined endpoint on an external Node.js server.\n    *   **Payload to Node.js Server:**\n        *   `agent_id`: The ID of the Letta agent initiating the request.\n        *   `memory_block_id`: The unique identifier of the Research Cache Block containing the raw search results.\n        *   `last_messages`: The last 2-3 turns of the conversation context leading up to the research request.\n        *   `user_query`: The specific query string used for the web search.\n5.  **Node.js Server Responsibilities:**\n    *   **Fetch Search Results:** The Node.js server uses the `memory_block_id` (via the Letta API as detailed in Section 4) to retrieve the full content of the Research Cache Block from the agent's memory system.\n    *   **Quality Control (QC) Invocation:**\n        *   The server sends the fetched search results, `last_messages`, and `user_query` to a designated Quality Control LLM (e.g., Gemini 2.0 Flash).\n        *   The QC LLM analyzes the search results against the provided conversational context and filters out irrelevant information, returning only the contextually relevant data.\n    *   **Graphiti Ingestion:** The Node.js server takes the filtered, relevant data from the QC LLM and ingests it into the Graphiti knowledge graph for long-term storage and recall.\n    *   **Memory Block Lifespan Management:**\n        *   The Node.js server monitors the age or usage of the Research Cache Block (e.g., by tracking conversation turns via the Letta API using `agent_id`, as detailed in Section 4).\n        *   After a predefined condition (e.g., 2 conversation turns have passed since its last update), the Node.js server deletes the Research Cache Block using the Letta API.\n6.  **Error Handling (Tool-Side):**\n    *   If the web search tool fails to make the API call to the Node.js server (e.g., network error, server unavailable), it will default to its legacy behavior: dumping the full, raw search results directly into the chat context to ensure the user still receives the information.\n\n## 3. System Components\n\n*   **Letta Agent:** Initiates and manages the interaction.\n*   **Web Search Tool:** Modified to store results in a memory block and call the external API.\n*   **Research Cache Block:** A temporary memory block holding raw search results.\n*   **Node.js Server (Dockerized):** Hosts the external logic for QC, Graphiti ingestion, and memory block lifecycle.\n*   **Gemini 2.0 Flash (or similar):** Acts as the QC LLM to filter search results.\n*   **Graphiti:** The long-term knowledge graph for storing processed information.\n\n## 4. Letta API Integration Details\n\nThe Node.js server will interact with the Letta API for several key operations:\n\n1.  **Fetching Conversation History for Turn Counting:**\n    *   **Endpoint (example):** `GET /v1/agents/{agent_id}/history` (Note: Specific endpoint for message/turn count might vary; this is a general history endpoint.)\n    *   **Purpose:** To retrieve conversation history or message counts for the specified `agent_id` to determine how many turns have passed since the Research Cache Block was last updated. This information is used to trigger the block's deletion.\n    *   **Parameters likely needed:** `agent_id`.\n\n2.  **Accessing and Managing the Research Cache Block:**\n    *   **Retrieve Block Content:** `GET /v1/agents/{agent_id}/core-memory/blocks/{block_label_or_id}`\n        *   **Purpose:** Used by the Node.js server to fetch the full content of the Research Cache Block using its `memory_block_id` (which might be its label or a unique ID) for processing by the QC LLM.\n    *   **Delete Block:** `DELETE /v1/agents/{agent_id}/core-memory/blocks/{block_label_or_id}`\n        *   **Purpose:** Used by the Node.js server to remove the Research Cache Block once its lifespan (determined by turn count) has expired.\n    *   *(Note: The web search tool itself will likely use `POST /v1/agents/{agent_id}/core-memory/blocks` or `PATCH /v1/agents/{agent_id}/core-memory/blocks/{block_label_or_id}` to initially create or update the Research Cache Block.)*\n\n3.  **Authentication:**\n    *   All API calls to the Letta API will need to be properly authenticated, likely using API keys or tokens managed by the Node.js server environment.\n\n## 5. Benefits\n\n*   **Reduced Context Usage:** Significantly lowers the data load in the agent's active context window.\n*   **Preservation of Full Data:** Raw search results are temporarily available if needed.\n*   **High-Quality Graphiti Ingestion:** LLM-based QC ensures only relevant information is stored long-term.\n*   **Automated Cleanup:** Temporary data is automatically managed and deleted.\n*   **Decoupled Architecture:** Externalizes complex processing, keeping the agent tool logic leaner.\n</code></pre>\n",
  "tags": [],
  "book": "Optimized Web Research",
  "chapter": ""
}